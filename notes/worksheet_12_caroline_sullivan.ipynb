{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 12\n",
    "\n",
    "Name:  Caroline Sullivan\n",
    "UID: U61353624 \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Naive Bayes\n",
    "- Model Evaluation\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "| Attribute A | Attribute B | Attribute C | Class |\n",
    "|-------------|-------------|-------------|-------|\n",
    "| Yes         | Single      | High        | No    |\n",
    "| No          | Married     | Mid         | No    |\n",
    "| No          | Single      | Low         | No    |\n",
    "| Yes         | Married     | High        | No    |\n",
    "| No          | Divorced    | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| Yes         | Divorced    | High        | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "| No          | Married     | Low         | No    |\n",
    "| No          | Single      | Mid         | Yes   |\n",
    "\n",
    "a) Compute the following probabilities:\n",
    "\n",
    "- P(Attribute A = Yes | Class = No) = 3/7\n",
    "- P(Attribute B = Divorced | Class = Yes) = 1/3\n",
    "- P(Attribute C = High | Class = No) = 3/7\n",
    "- P(Attribute C = Mid | Class = Yes) = 3/3 = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3/7\n",
    "- 1/3\n",
    "- 3/7\n",
    "- 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Classify the following unseen records:\n",
    "\n",
    "- (Yes, Married, Mid) need to do P(Yes) * P(yes | C=Y) * P(married | C=Y) * P(mid | C=Y) VS. P(No) P(yes | C=N) * P(married | C=N) * P(mid | C=N)\n",
    "- (No, Divorced, High)\n",
    "- (No, Single, High)\n",
    "- (No, Divorced, Low)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For YES, there is a 0 probability, so overall 0. We therefore predict class NO.\n",
    "2. For YES: 1 * 1/3 * 0. For NO, no zeroes. Predicted class NO.\n",
    "3. For YES: 1 * 2/3 * 0. For NO, no zeroes. Predicted class NO.\n",
    "4. For YES: 1 * 1/3 * 0. For NO, no zeroes. Predicted class NO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "a) Write a function to generate the confusion matrix for a list of actual classes and a list of predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t predicted=Y  Predicted=N\n",
      "actual=Y\t2\t 1\n",
      "actual=N\t3\t 4\n"
     ]
    }
   ],
   "source": [
    "actual_class = [\"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"]\n",
    "predicted_class = [\"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == \"Yes\" and predicted[i] == \"Yes\":\n",
    "            TP += 1\n",
    "        elif actual[i] == \"No\" and predicted[i] == \"No\":\n",
    "            TN += 1\n",
    "        if actual[i] == \"Yes\" and predicted[i] == \"No\":\n",
    "            FN += 1\n",
    "        if actual[i] == \"No\" and predicted[i] == \"Yes\":\n",
    "            FP += 1\n",
    "    \n",
    "    cm = \"\\t predicted=Y  Predicted=N\\nactual=Y\\t%s\\t %s\\nactual=N\\t%s\\t %s\" % (TP, FN, FP, TN)\n",
    "\n",
    "    return cm\n",
    "\n",
    "print(confusion_matrix(actual_class, predicted_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume you have the following Cost Matrix:\n",
    "\n",
    "|            | predicted = Y | predicted = N |\n",
    "|------------|---------------|---------------|\n",
    "| actual = Y |       -1      |       5       |\n",
    "| actual = N |        10     |       0       |\n",
    "\n",
    "What is the cost of the above classification?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write a function that takes in the actual values, the predictions, and a cost matrix and outputs a cost. Test it on the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 33\n"
     ]
    }
   ],
   "source": [
    "test_cost_mat = {'TP':-1, 'FN':5, 'FP':10, 'TN':0}\n",
    "\n",
    "def predict_cost(actual, predicted, cost_mat):\n",
    "    # initialize confusion matrix\n",
    "    cm = {'TP':0, 'FN':0, 'FP':0, 'TN':0}\n",
    "\n",
    "    # initialize cost\n",
    "    cost = 0\n",
    "\n",
    "    # build confusion mat (could have used previous function if I had just made it return a dictionary)\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == \"Yes\" and predicted[i] == \"Yes\":\n",
    "            cm['TP'] += 1\n",
    "        elif actual[i] == \"No\" and predicted[i] == \"No\":\n",
    "            cm['TN'] += 1\n",
    "        if actual[i] == \"Yes\" and predicted[i] == \"No\":\n",
    "            cm['FN'] += 1\n",
    "        if actual[i] == \"No\" and predicted[i] == \"Yes\":\n",
    "            cm['FP'] += 1\n",
    "    \n",
    "    # add up the cost\n",
    "    for confusion in cm:\n",
    "        cost += cm[confusion] * cost_mat[confusion]\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# test\n",
    "print('Cost:', predict_cost(actual_class, predicted_class, test_cost_mat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) What is the difference between a testing set and a validation set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A testing set is the portion of the dataset that we withold during the training phase. We train the model using the train portion of the dataset, then we can test the model on data it hasn't seen yet, AKA the test set. This gives a \"preview\" of how the model might perform on outside data & can help ensure we are not overfitting to the training set. The test data must be completely separate, because if you try testing the model on data it has already seen, that is cheating!\n",
    "- A validation set is also a portion of the data set separate from the test set that can be witheld during training. The purpose of this data is specifically to tune parameters of the model. If you use a validation set, you should still also use a test set after you have settled on the final parameters to make sure you haven't overfit to the validation dataset. Once you move on to the test set, you can't go back and change the hyperparameters, as this would also be cheating."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) What are some things you can do to handle the case where you have a highly imbalanced dataset (i.e. there are way more of one class than another). Describe both how you can provide better visibility into the failures of the model and how you can set your model training up for success."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have really imbalanced data, it can be difficult to tell how well your model is actually performing. For example, if the purpose of my model is to distinguish between apples and bananas, and my dataset is 90 apples & 10 bananas, the model will still perform at 90% accuracy if it predicts everything to be an apple. This doesn't mean it's a great model though because it clearly doesn't do a good job picking out bananas.\n",
    "\n",
    "One way to combat this is to create a better metric than accuracy, such as a cost, as shown above. This can help give a better picture of where the model is failing. For example, if developing a test for a disease like COVID, heavily penalizing false negatives would be a good choice when evaluating the test, whereas true positives should lower the cost. If disease prevalence is low, this is especially important because that is essentially a version of an imbalanced dataset since a vast majority of people are healthy.\n",
    "\n",
    "To make better predictions, you could also aggregate the results of multiple classifiers. To do this, you need to find a way to create independent datasets for training. This can be done via bootstrapping, which involves repeatedly sampling from your dataset with replacement. Boosting works similarly but also helps prioritize picking datasets weighted towards the datapoints that are more difficult to classify, thus \"forcing\" the model to learn how to classify those points. It also weights the models themselves by accuracy. This way, the final classification is done using a weighted majority approach, which could also lead to a more successful final ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
